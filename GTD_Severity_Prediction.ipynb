{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vncDsAP0Gaoa"
      },
      "source": [
        "# **Project Name**    - GTD Severity Prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beRrZCGUAJYm"
      },
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### Kanika Singh Rajpoot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJNUwmbgGyua"
      },
      "source": [
        "# **Project Summary -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6v_1wHtG2nS"
      },
      "source": [
        "\n",
        "Build an end-to-end machine learning pipeline to classify terrorist incidents from the Global Terrorism Database (GTD) into three severity levels (Low, Medium, High), based on casualties and incident characteristics.\n",
        "\n",
        "The model uses both:\n",
        "- Unstructured text (attack summaries).\n",
        "- Structured metadata (attack type, weapon type, target type, success flag, suicide flag, coordinates).\n",
        "\n",
        "---\n",
        "\n",
        "### Data Preparation\n",
        "\n",
        "- Source: Global Terrorism Database (GTD).\n",
        "- Selected key fields:\n",
        "  - `summary` (free-text description of the incident)\n",
        "  - `nkill` (number killed)\n",
        "  - `nwound` (number wounded)\n",
        "  - `attacktype1_txt` (attack type)\n",
        "  - `weaptype1_txt` (weapon type)\n",
        "  - `targtype1_txt` (target type)\n",
        "  - `region_txt`, `country_txt`\n",
        "  - `success`, `suicide`, `latitude`, `longitude`\n",
        "\n",
        "- Created a custom severity target:\n",
        "  - `casualties = nkill + nwound` (after filling missing with 0)\n",
        "  - Severity classes:\n",
        "    - Low: casualties <= 2\n",
        "    - Medium: 3 <= casualties <= 10\n",
        "    - High: casualties > 10\n",
        "\n",
        "- Basic preprocessing:\n",
        "  - Filled missing text summaries with empty strings.\n",
        "  - Handled missing values in key categorical and numeric fields.\n",
        "\n",
        "---\n",
        "\n",
        "### Text Preprocessing and Feature Engineering\n",
        "\n",
        "1. Baseline text cleaning:\n",
        "   - Lowercasing.\n",
        "   - Punctuation removal.\n",
        "   - Digit removal.\n",
        "   - Stopword removal using NLTK English stopwords.\n",
        "\n",
        "2. Domain-specific stopwords:\n",
        "   - Removed high-frequency but low-information words such as:\n",
        "     - \"responsibility\", \"claimed\", \"group\", \"attack\", \"incident\",\n",
        "       \"assailants\", \"device\", \"city\", \"province\", \"district\", \"reported\",\n",
        "       \"sources\", \"area\", \"unknown\", \"near\", \"people\", \"state\", \"forces\", \"police\"\n",
        "   - Goal: reduce noise and keep words that are more related to severity patterns.\n",
        "\n",
        "3. Token engineering for severity-related phrases:\n",
        "   - Added custom tokens when certain patterns appear:\n",
        "     - \"suicide\" + \"bomb\" → `token_suicide_bomb`\n",
        "     - \"car bomb\" → `token_car_bomb`\n",
        "     - \"truck bomb\" → `token_truck_bomb`\n",
        "     - \"hostage\" → `token_hostage`\n",
        "   - These engineered tokens act as strong features for high-impact events.\n",
        "\n",
        "4. Metadata-aware text field:\n",
        "   - Built a combined text field:\n",
        "     - `text_v5 = clean_summary_with_tokens + attacktype1_txt + weaptype1_txt + targtype1_txt`\n",
        "   - This allows TF-IDF to capture patterns across both summary and metadata descriptions.\n",
        "\n",
        "---\n",
        "\n",
        "### Feature Construction\n",
        "\n",
        "- Categorical features:\n",
        "  - Columns: `region_txt`, `country_txt`, `attacktype1_txt`, `weaptype1_txt`, `targtype1_txt`\n",
        "  - Encoded using `OneHotEncoder(handle_unknown=\"ignore\")`.\n",
        "\n",
        "- Structured numerical features:\n",
        "  - `success`, `suicide`, `latitude`, `longitude`\n",
        "  - Converted to sparse matrices and stacked with other features.\n",
        "\n",
        "- Text features (final version TF-IDF V5):\n",
        "  - `TfidfVectorizer` on `text_v5` with:\n",
        "    - `max_features = 3000`\n",
        "    - `ngram_range = (1, 2)` (unigrams + bigrams)\n",
        "    - `min_df = 2` (ignore extremely rare tokens)\n",
        "    - `sublinear_tf = True` (log-scaling of term frequency)\n",
        "\n",
        "- Final feature matrix:\n",
        "  - Horizontal stack of:\n",
        "    - One-hot encoded categorical features.\n",
        "    - Structured numerical features.\n",
        "    - TF-IDF text features.\n",
        "\n",
        "---\n",
        "\n",
        "### Modeling\n",
        "\n",
        "Trained and compared multiple models with the sparse feature matrix:\n",
        "\n",
        "- Logistic Regression (baseline and tuned).\n",
        "- Linear Support Vector Machine (LinearSVC).\n",
        "- Passive-Aggressive Classifier.\n",
        "- Complement Naive Bayes.\n",
        "- RidgeClassifier.\n",
        "- Word2Vec + ML (dense embeddings).\n",
        "- MiniLM sentence embeddings + ML.\n",
        "- Hybrid TF-IDF + sentence embeddings (advanced extension).\n",
        "\n",
        "Hyperparameters tuned included:\n",
        "- Regularization strength `C`.\n",
        "- Class weights (e.g., `class_weight=\"balanced\"`).\n",
        "- Loss functions for SVM (`hinge`, `squared_hinge`).\n",
        "- TF-IDF settings: `max_features`, `min_df`, `ngram_range`, `sublinear_tf`.\n",
        "\n",
        "---\n",
        "\n",
        "### Best Model\n",
        "\n",
        "The best performing model was:\n",
        "\n",
        "- Model: Linear SVM (LinearSVC)\n",
        "- Features: TF-IDF V5 + one-hot categorical + structured numeric.\n",
        "- Approximate performance:\n",
        "  - Accuracy: around 0.86\n",
        "  - Macro F1: around 0.77\n",
        "  - Good balance across classes, with strong performance on Medium and High severity.\n",
        "\n",
        "Key reasons for good performance:\n",
        "- Rich text representation that combines summary and metadata.\n",
        "- Domain-specific cleaning and custom tokens for severity-related patterns.\n",
        "- Linear SVM works well with high-dimensional sparse features.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Insights\n",
        "\n",
        "- Text summaries carry strong signal for severity, especially when enriched with:\n",
        "  - Attack type text.\n",
        "  - Weapon type text.\n",
        "  - Target type text.\n",
        "  - Custom severity tokens (suicide bomb, car bomb, hostage, etc.).\n",
        "- Location features (country, region) have limited direct value for severity prediction.\n",
        "- Classical sparse models (TF-IDF + Linear SVM) can outperform generic deep embeddings on this kind of short, structured text.\n",
        "\n",
        "---\n",
        "\n",
        "### Future Work\n",
        "\n",
        "- Fine-tune transformer models (e.g., BERT, RoBERTa) on GTD summaries for potentially better representation of context and severity cues.\n",
        "- Add calibrated probability outputs instead of hard class labels for use in risk scoring.\n",
        "- Introduce time and sequence modeling (temporal patterns, escalation trends).\n",
        "- Improve explainability using feature importance and local explanation methods.\n",
        "- Wrap the model into an API or interactive app for practical usage (for example, using FastAPI or Streamlit).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6K7xa23Elo4"
      },
      "source": [
        "# **GitHub Link -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQaldy8SH6Dl"
      },
      "source": [
        "# **Problem Statement**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpeJGUA3kjGy"
      },
      "source": [
        "\n",
        "The Global Terrorism Database (GTD) contains detailed records of terrorist incidents worldwide, including free-text summaries and structured metadata such as attack type, weapon type, target type, location, and casualty counts.\n",
        "\n",
        "However, there is no ready-to-use field that categorizes attacks into interpretable severity levels. For practical risk assessment, law enforcement and intelligence teams often need a quick indication of how severe an incident is likely to be (for example, low-impact vs. mass-casualty events).\n",
        "\n",
        "The goal of this project is to build a machine learning model that:\n",
        "\n",
        "- Uses both textual summaries and structured features from the GTD.\n",
        "- Automatically classifies each incident into a severity category:\n",
        "  - Low severity\n",
        "  - Medium severity\n",
        "  - High severity\n",
        "- Is robust enough to generalize to unseen incidents and support early-warning or prioritization workflows.\n",
        "\n",
        "This requires:\n",
        "- Defining a meaningful severity label based on casualties.\n",
        "- Combining NLP-based features from the attack summary with structured metadata.\n",
        "- Training and evaluating suitable machine learning models for multi-class classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_i_v8NEhb9l"
      },
      "source": [
        "# ***Let's Begin !***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhfV-JJviCcP"
      },
      "source": [
        "## ***1. Know Your Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install dependencies**"
      ],
      "metadata": {
        "id": "BHwNl4JhHdAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q joblib\n",
        "!pip install -q nltk\n"
      ],
      "metadata": {
        "id": "-eCLlFICHeK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3lxredqlCYt"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re, string\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import joblib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RnN4peoiCZX"
      },
      "source": [
        "### Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "FILEPATH = '/content/drive/My Drive/projects/global impact/Global Terrorism Data.csv'\n",
        "df = pd.read_csv(FILEPATH, encoding='latin1', low_memory=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x71ZqKXriCWQ"
      },
      "source": [
        "### Explore dataset basic info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "outputs": [],
      "source": [
        "print(\"Shape:\", df.shape)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hBIi_osiCS2"
      },
      "source": [
        "### Dataset Rows & Columns count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "outputs": [],
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlHwYmJAmNHm"
      },
      "source": [
        "### Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "outputs": [],
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include='all').T\n"
      ],
      "metadata": {
        "id": "WloSdb1BI2Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoPl-ycgm1ru"
      },
      "source": [
        "#### Missing Values/Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "outputs": [],
      "source": [
        "# Missing Values/Null Values Count\n",
        "key_cols = ['nkill','nwound','summary','weaptype1_txt',\n",
        "            'attacktype1_txt','region_txt','country_txt']\n",
        "\n",
        "df[key_cols].isnull().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWDCXeEa19cg"
      },
      "outputs": [],
      "source": [
        "df['casualties'] = df[['nkill','nwound']].fillna(0).sum(axis=1)\n",
        "df['casualties'].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km2F6axr2C5a"
      },
      "outputs": [],
      "source": [
        "df['casualties'].describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFPGwF41320I"
      },
      "outputs": [],
      "source": [
        "# Create severity class\n",
        "def severity_label(x):\n",
        "    if x <= 2:\n",
        "        return \"Low\"\n",
        "    elif 3 <= x <= 10:\n",
        "        return \"Medium\"\n",
        "    else:\n",
        "        return \"High\"\n",
        "\n",
        "df['severity'] = df['casualties'].apply(severity_label)\n",
        "\n",
        "df[['casualties','severity']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esLB5-qD4a2B"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum().sort_values(ascending=False).head(20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXfrZWBD4iHF"
      },
      "source": [
        "**Distribution of Severity Classes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "my1OD3iV4cxb"
      },
      "outputs": [],
      "source": [
        "df['severity'].value_counts().plot(kind='bar', figsize=(6,4), color=['green','orange','red'])\n",
        "plt.title(\"Severity Class Distribution\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnhDlw1r4pbm"
      },
      "source": [
        "**Casualty Histogram**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GKcCx9A4n7b"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df['casualties'], bins=100, kde=False)\n",
        "plt.xlim(0,50)   # limit for visibility (tail is huge)\n",
        "plt.title(\"Casualty Distribution (0–50)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03FXBBaD4yOT"
      },
      "source": [
        "**Yearly Attacks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TE2UjgPq4ztH"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "df['iyear'].value_counts().sort_index().plot()\n",
        "plt.title(\"Attacks per Year\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mGjKuSW42iU"
      },
      "source": [
        "**Region-wise Attacks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xN2laaDe44wq"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "df['region_txt'].value_counts().plot(kind=\"bar\")\n",
        "plt.title(\"Attacks by Region\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGqgbmdm5GZF"
      },
      "source": [
        "**Attack Type Distribution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiwMZavW5Gql"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "df['attacktype1_txt'].value_counts().plot(kind=\"bar\")\n",
        "plt.title(\"Attack Type Distribution\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6zsagVY5Vyr"
      },
      "source": [
        "*Weapon Type Distribution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E49DMYcV5RaN"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "df['weaptype1_txt'].value_counts().plot(kind=\"bar\")\n",
        "plt.title(\"Weapon Type Distribution\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0z-Kja45iKV"
      },
      "source": [
        "**Target Type Distribution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbiULhFZ5k2K"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "df['targtype1_txt'].value_counts().plot(kind=\"bar\")\n",
        "plt.title(\"Target Type Distribution\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ4tUY4T5x64"
      },
      "source": [
        "**Summary Text Length Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBmI4Xhl5zFa"
      },
      "outputs": [],
      "source": [
        "df['text_length'] = df['summary'].fillna(\"\").apply(lambda x: len(x.split()))\n",
        "sns.histplot(df['text_length'], bins=50)\n",
        "plt.title(\"Distribution of Summary Text Length\")\n",
        "plt.xlim(0,200)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYJBCPmzokLo"
      },
      "source": [
        "# **DATA CLEANING AND PREPRCESSING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMaxQxLFoRmk"
      },
      "source": [
        "**0: Copy Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPBGsc28oQtu"
      },
      "outputs": [],
      "source": [
        "data = df.copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wiz5D6pWvOEt"
      },
      "source": [
        "**1 — Clean Missing Values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-npHBDGvPDX"
      },
      "outputs": [],
      "source": [
        "df[\"summary_len\"] = df[\"summary\"].fillna(\"\").apply(lambda x: len(str(x).split()))\n",
        "df[\"summary_len\"].describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Cleaning"
      ],
      "metadata": {
        "id": "p44xTZxmKB1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    words = [w for w in text.split() if w not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "df[\"summary\"] = df[\"summary\"].fillna(\"\")\n",
        "df[\"clean_summary\"] = df[\"summary\"].apply(clean_text)\n",
        "df[\"clean_summary\"].head()\n"
      ],
      "metadata": {
        "id": "mbYp79lqKCnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove domain stopwords"
      ],
      "metadata": {
        "id": "NY5nfDVtKH4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "domain_stopwords = {\n",
        "    \"responsibility\",\"claimed\",\"group\",\"attack\",\"incident\",\"assailants\",\n",
        "    \"device\",\"city\",\"province\",\"district\",\"reported\",\"sources\",\"area\",\n",
        "    \"unknown\",\"near\",\"three\",\"two\",\"one\",\"people\",\"state\",\"forces\",\"police\"\n",
        "}\n",
        "\n",
        "def clean_text_v2(text):\n",
        "    words = text.split()\n",
        "    return \" \".join([w for w in words if w not in domain_stopwords])\n",
        "\n",
        "df[\"clean_summary_v2\"] = df[\"clean_summary\"].apply(clean_text_v2)\n"
      ],
      "metadata": {
        "id": "cvKTUquQKGI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Token Engineering"
      ],
      "metadata": {
        "id": "K6NHMRmLKKFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_tokens(text):\n",
        "    t = text.lower()\n",
        "    if \"suicide\" in t and \"bomb\" in t:\n",
        "        text += \" token_suicide_bomb\"\n",
        "    if \"car bomb\" in t:\n",
        "        text += \" token_car_bomb\"\n",
        "    if \"truck bomb\" in t:\n",
        "        text += \" token_truck_bomb\"\n",
        "    if \"hostage\" in t:\n",
        "        text += \" token_hostage\"\n",
        "    return text\n",
        "\n",
        "df[\"clean_summary_v3\"] = df[\"clean_summary_v2\"].apply(add_tokens)\n"
      ],
      "metadata": {
        "id": "tu4mQPq5KKuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build combined text field"
      ],
      "metadata": {
        "id": "114H1spNKO2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in [\"attacktype1_txt\",\"weaptype1_txt\",\"targtype1_txt\"]:\n",
        "    df[col] = df[col].fillna(\"Unknown\").astype(str)\n",
        "\n",
        "df[\"text_v5\"] = (\n",
        "    df[\"clean_summary_v3\"] + \" \" +\n",
        "    df[\"attacktype1_txt\"] + \" \" +\n",
        "    df[\"weaptype1_txt\"] + \" \" +\n",
        "    df[\"targtype1_txt\"]\n",
        ")\n",
        "\n",
        "df[\"text_v5\"].head()\n"
      ],
      "metadata": {
        "id": "UUIDugN2KPXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-test split"
      ],
      "metadata": {
        "id": "E1FxvtJjKS5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.copy()\n",
        "y = df[\"severity\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n"
      ],
      "metadata": {
        "id": "KIwSpzYMKTix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-hot encode categorical features"
      ],
      "metadata": {
        "id": "Y620n13xKXSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = [\"region_txt\",\"country_txt\",\"attacktype1_txt\",\"weaptype1_txt\",\"targtype1_txt\"]\n",
        "\n",
        "for col in categorical_cols:\n",
        "    X_train[col] = X_train[col].fillna(\"Unknown\")\n",
        "    X_test[col]  = X_test[col].fillna(\"Unknown\")\n",
        "\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=True)\n",
        "\n",
        "X_train_cat = ohe.fit_transform(X_train[categorical_cols])\n",
        "X_test_cat  = ohe.transform(X_test[categorical_cols])\n"
      ],
      "metadata": {
        "id": "tpMyWCFIKX4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Structured features"
      ],
      "metadata": {
        "id": "YVsSivgBKawU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "structured_features = [\"success\",\"suicide\",\"latitude\",\"longitude\"]\n",
        "\n",
        "X_train_struct = csr_matrix(X_train[structured_features].fillna(0).values)\n",
        "X_test_struct  = csr_matrix(X_test[structured_features].fillna(0).values)\n"
      ],
      "metadata": {
        "id": "V6UisV9iKbK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF Final Vectorizer"
      ],
      "metadata": {
        "id": "stE7R4KoKdEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_final = TfidfVectorizer(\n",
        "    max_features=3000,\n",
        "    min_df=2,\n",
        "    ngram_range=(1,2),\n",
        "    sublinear_tf=True\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf_final.fit_transform(X_train[\"text_v5\"])\n",
        "X_test_tfidf  = tfidf_final.transform(X_test[\"text_v5\"])\n"
      ],
      "metadata": {
        "id": "3IhlAseKKex4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final feature matrix"
      ],
      "metadata": {
        "id": "w_dUnl9AKmB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_final = hstack([X_train_cat, X_train_struct, X_train_tfidf])\n",
        "X_test_final  = hstack([X_test_cat,  X_test_struct,  X_test_tfidf])\n"
      ],
      "metadata": {
        "id": "Aojfikn3KmdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train FINAL Linear SVM"
      ],
      "metadata": {
        "id": "nR3Gi5rCKoPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_final = LinearSVC(\n",
        "    C=0.5,\n",
        "    loss=\"hinge\",\n",
        "    class_weight=\"balanced\"\n",
        ")\n",
        "\n",
        "svm_final.fit(X_train_final, y_train)\n",
        "y_pred = svm_final.predict(X_test_final)\n",
        "\n",
        "print(\"===== FINAL SVM PERFORMANCE =====\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "HkB_53r6KqCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save all artifacts"
      ],
      "metadata": {
        "id": "I81IqgnxKtno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# joblib.dump(tfidf_final, \"tfidf_v5.joblib\")\n",
        "# joblib.dump(ohe, \"ohe_encoder.joblib\")\n",
        "# joblib.dump(svm_final, \"svm_final_v5.joblib\")\n",
        "# joblib.dump(categorical_cols, \"categorical_cols.pkl\")\n",
        "# joblib.dump(structured_features, \"structured_features.pkl\")\n"
      ],
      "metadata": {
        "id": "GTHnDQO7KvNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction function"
      ],
      "metadata": {
        "id": "kDwmf6dCKyd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_severity(summary_text, row_df):\n",
        "    \"\"\"\n",
        "    summary_text : raw summary string\n",
        "    row_df       : a 1-row DataFrame with categorical + structured features\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Clean text\n",
        "    clean = clean_text(summary_text)\n",
        "\n",
        "    # 2) TF-IDF transform\n",
        "    tfidf_vec = tfidf_final.transform([clean])\n",
        "\n",
        "    # 3) OHE transform\n",
        "    ohe_vec = ohe.transform(row_df[categorical_cols])\n",
        "\n",
        "    # 4) Structured features\n",
        "    struct_vec = csr_matrix(row_df[structured_features].values)\n",
        "\n",
        "    # 5) Combine\n",
        "    final_vec = hstack([ohe_vec, struct_vec, tfidf_vec])\n",
        "\n",
        "    # 6) Predict\n",
        "    pred = svm_final.predict(final_vec)[0]\n",
        "\n",
        "    return pred\n"
      ],
      "metadata": {
        "id": "GFwdoo2HK0Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 1**"
      ],
      "metadata": {
        "id": "ghsPCnPuOFAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suicide bombing (should be HIGH)\n",
        "\n",
        "summary = \"A suicide bomber detonated explosives at a crowded market killing several civilians.\"\n",
        "\n",
        "row = pd.DataFrame([{\n",
        "    \"region_txt\": \"South Asia\",\n",
        "    \"country_txt\": \"Pakistan\",\n",
        "    \"attacktype1_txt\": \"Bombing/Explosion\",\n",
        "    \"weaptype1_txt\": \"Explosives\",\n",
        "    \"targtype1_txt\": \"Private Citizens & Property\",\n",
        "\n",
        "    \"success\": 1,\n",
        "    \"suicide\": 1,\n",
        "    \"latitude\": 33.6,\n",
        "    \"longitude\": 71.5\n",
        "}])\n",
        "\n",
        "predict_severity(summary, row)\n",
        "\n"
      ],
      "metadata": {
        "id": "9NiQurxQOHOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXAMPLE 2"
      ],
      "metadata": {
        "id": "0Wu4-IyMOZ3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Small arms attack, few injuries (Medium/Low)\n",
        "\n",
        "summary = \"Gunmen opened fire on a police patrol injuring two officers before fleeing the scene.\"\n",
        "\n",
        "row = pd.DataFrame([{\n",
        "    \"region_txt\": \"Middle East & North Africa\",\n",
        "    \"country_txt\": \"Iraq\",\n",
        "    \"attacktype1_txt\": \"Armed Assault\",\n",
        "    \"weaptype1_txt\": \"Firearms\",\n",
        "    \"targtype1_txt\": \"Police\",\n",
        "\n",
        "    \"success\": 1,\n",
        "    \"suicide\": 0,\n",
        "    \"latitude\": 34.2,\n",
        "    \"longitude\": 44.4\n",
        "}])\n",
        "\n",
        "predict_severity(summary, row)\n"
      ],
      "metadata": {
        "id": "HDoDQy2-Oab7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXAMPLE 3"
      ],
      "metadata": {
        "id": "tjQa3pqAOhif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Failed attack, no casualties\n",
        "summary = \"An explosive device was planted near a government building but failed to detonate.\"\n",
        "\n",
        "row = pd.DataFrame([{\n",
        "    \"region_txt\": \"Western Europe\",\n",
        "    \"country_txt\": \"United Kingdom\",\n",
        "    \"attacktype1_txt\": \"Bombing/Explosion\",\n",
        "    \"weaptype1_txt\": \"Explosives\",\n",
        "    \"targtype1_txt\": \"Government (General)\",\n",
        "\n",
        "    \"success\": 0,\n",
        "    \"suicide\": 0,\n",
        "    \"latitude\": 51.5,\n",
        "    \"longitude\": -0.1\n",
        "}])\n",
        "\n",
        "predict_severity(summary, row)\n",
        "\n"
      ],
      "metadata": {
        "id": "042SIUCvOkhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "# Save trained SVM model\n",
        "joblib.dump(svm_final, \"svm_final.joblib\")\n",
        "\n",
        "# Save TF-IDF vectorizer (final tuned one)\n",
        "joblib.dump(tfidf_final, \"tfidf_vectorizer.joblib\")\n",
        "\n",
        "# Save OneHotEncoder\n",
        "joblib.dump(ohe, \"ohe_encoder.joblib\")\n",
        "\n",
        "print(\"All model files saved!\")\n"
      ],
      "metadata": {
        "id": "rqDYy-OWbp9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"svm_final.joblib\")\n",
        "files.download(\"tfidf_vectorizer.joblib\")\n",
        "files.download(\"ohe_encoder.joblib\")\n"
      ],
      "metadata": {
        "id": "xjAEbtCehXKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Future Scope**"
      ],
      "metadata": {
        "id": "douqvD3YUfs2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Fine-tune transformer models (e.g., BERT, RoBERTa) on GTD summaries for potentially better representation of context and severity cues.\n",
        "- Add calibrated probability outputs instead of hard class labels for use in risk scoring.\n",
        "- Introduce time and sequence modeling (temporal patterns, escalation trends).\n",
        "- Improve explainability using feature importance and local explanation methods.\n",
        "- Wrap the model into an API or interactive app for practical usage (for example, using FastAPI or Streamlit)."
      ],
      "metadata": {
        "id": "FqPbqAa6Uotp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCX9965dhzqZ"
      },
      "source": [
        "# **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      },
      "source": [
        "In this project, we developed a machine learning system to predict the severity level of terrorist attacks (Low, Medium, High) based on structured metadata and unstructured text summaries from the Global Terrorism Database (GTD).\n",
        "This project combined NLP, feature engineering, sparse linear modeling, and rigorous evaluation.\n",
        "\n",
        "**Key Outcomes**\n",
        "\n",
        "- Built a comprehensive end-to-end ML pipeline: cleaning → feature engineering → TF-IDF modeling → hyperparameter tuning.\n",
        "- Designed a custom severity label from real-world casualty counts.\n",
        "- Engineered multiple generations of text representations:\n",
        "  - Baseline TF-IDF (V1)\n",
        "  - Domain-specific stopwords (V2)\n",
        "  - Balanced TF-IDF (V3)\n",
        "  - Meaningful terrorism n-grams (V4)\n",
        "  - Metadata-enriched TF-IDF (V5)\n",
        "\n",
        "- Trained and compared several sparse-friendly models:\n",
        "  - Logistic Regression\n",
        "  - Linear SVM\n",
        "  - Passive-Aggressive\n",
        "  - Complement Naive Bayes\n",
        "  - Ridge Classifier\n",
        "  - Word2Vec\n",
        "  - MiniLM Embeddings\n",
        "  - Hybrid SBERT + TF-IDF model\n",
        "\n",
        "**Final Best Model**\n",
        "\n",
        "Linear SVM + TF-IDF V5 (metadata-aware and phrase-enhanced)\n",
        "Macro F1 ≈ 0.77\n",
        "Accuracy ≈ 0.86\n",
        "\n",
        "The model learned strong predictive patterns from descriptive summaries, attack type, weapon type, and engineered terrorism-specific tokens.\n",
        "\n",
        "It can now reliably classify unseen attacks into severity levels and can be deployed as a risk assessment tool."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "yQaldy8SH6Dl",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "PoPl-ycgm1ru"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}